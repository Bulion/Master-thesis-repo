\documentclass[printmode,en]{mgr}
%opcje klasy dokumentu mgr.cls zostały opisane w dołączonej instrukcji

%poniżej deklaracje użycia pakietów, usunąć to co jest niepotrzebne
%\usepackage{polski}       %przydatne podczas składania dokumentów w
%j. polskim
\usepackage[english]{babel} %alternatywnie do pakietu
%polski, wybrać jeden z nich
\usepackage[utf8]{inputenc} %kodowanie znaków, zależne od systemu
\usepackage[T1]{fontenc} %poprawne składanie polskich czcionek

\usepackage{eurosym}

\usepackage{hyperref}

%pakiety do grafiki
\usepackage{graphicx}
\usepackage{psfrag}
\usepackage{subcaption}
\usepackage{lscape}

\graphicspath{{../graphics/}}

%pakiety dodające dużo dodatkowych poleceń matematycznych
\usepackage{amsmath}
\usepackage{amsfonts}

%pakiety wspomagające i poprawiające składanie tabel
\usepackage{supertabular}
\usepackage{array}
\usepackage{tabularx}
\usepackage{hhline}

%pakiet wypisujący na marginesie etykiety równań i rysunków
%zdefiniowanych przez \label{}, chcąc wygenerować finalną wersję
%dokumentu wystarczy usunąć poniższą linię
%\usepackage{showlabels}

%definicje własnych poleceń
\newcommand{\R}{I\!\!R} %symbol liczb rzeczywistych, działa tylko w
                        %trybie matematycznym
\newtheorem{theorem}{Twierdzenie}[section] %nowe otoczenie do
                                           %składania twierdzeń

%dane do złożenia strony tytułowej
\title{Zastosowanie okularów rozszerzonej rzeczywistości w aplikacjach
robotycznych}
\engtitle{Augmented reality goggles in robotic applications}
\author{Dawid Śliwa}
\supervisor{dr inż. Janusz Jakubiak, I-6}
%\guardian{dr hab. inż. Imię Nazwisko Prof. PWr, I-6} %nie używać
%jeśli opiekun jest tą samą osobą co prowadzący pracę

%\date{2008} %standardowo u dołu strony tytułowej umieszczany jest
%bieżący rok, to polecenie pozwala wstawić dowolny rok

%poniżej jest lista kierunków i specjalności na wydziale elektroniki,
%należy wybrać właściwe lub dopisać jeśli nie ma odpowiednich
\field{Control Engineering and Robotics (AIR)}
\specialisation{Embedded Robotics (AER)}
%\specialisation{Komputerowe sieci sterowania (ARK)}
%\specialisation{Systemy informatyczne w automatyce (ASI)}
%\specialisation{Komputerowe systemy zarządzania \\procesami
%produkcyjnymi (ARS)} \field{Elektronika i telekomunikacja (EIT)}
%\specialisation{Akustyka (ETA)} \specialisation{Aparatura
%elektroniczna (EAE)} \specialisation{Elektroniczne i komputerowe
%\\systemy automatyki (ESA)} \specialisation{Zastosowania inżynierii
%komputerowej \\w technice (EZI)} \specialisation{Inżynieria dźwięku
%(EID)} \specialisation{Elektronika stosowana \\i optokomunikacja
%(TEO)} \specialisation{Telekomunikacyjne sieci szerokopasmowe (TSS)}
%\specialisation{Teleinformatyczne sieci mobilne (TSM)}
%\specialisation{Sygnały w telekomunikacji cyfrowej (TSC)}
%\specialisation{Teleinformatyczne systemy rozsiewcze (TSR)}
%\field{Informatyka (INF)} \specialisation{Systemy informatyki w
%medycynie \\i technice (IMT)} \specialisation{Inżynieria systemów
%informatycznych (INS)} \specialisation{Inżynieria internetowa (INT)}
%\specialisation{Systemy i sieci komputerowe (ISK)}
%\field{Teleinformatyka (TIN)} \specialisation{Teleinformatyka (TIN)}

%tutaj zaczyna się właściwa treść dokumentu
\begin{document}
\bibliographystyle{plabbrv} %tylko gdy używamy BibTeXa, ustawia polski
                            %styl bibliografii

\maketitle %polecenie generujące stronę tytułową

%\dedication{6cm}{To jest przykładowa treść opcjonalnej dedykacji,
%  należy ją zmienić lub usunąć w całości polecenie
%  \texttt{$\backslash$dedication}}

\tableofcontents %spis treści

\chapter{Introduction}
Robots become more an more often seen in our environment. Starting from nowadays standard industrial applications and ending on home appliances robots. They all have more or less user friendly interface created to program or control them. In factories can be seen most often stationary or hand-held controllers and in consumer appliances, smartphone almost every time is used. Problem is that, this kind of interaction is not natural for humans. For comparison, communication between two employees working together is mostly done by voice, gestures and sometime touch. That is why modern controllers should been using these. This could improve a way of interaction on human-machine level. \\

A few years ago, a revolution called Industry 4.0 began which most important statement was to not replace peoples in factories by machines, but allow them to cooperate at production line. From that time companies are trying to simplify teaching process of robots and give them ability to sense the changing environment. Also enhancements are done on the other side. Employees are equipped with many solutions with are extending they perception. This is allowing to get better understanding what machines are doing or even see what they are "thinking".

\section{Purpose and scope of work}
This thesis will focus on Augmented Reality and they usage in modern factories and research facilities. At the beginning, different types of AR technologies will be compared to give overall view on how this is working. Then industrial or commercial products which are available right now on the market will be presented. The last part of topic studies will try to present selected solutions with are already used in real world applications.\\

Research part of this thesis will try to present simple examples of implementation AR in robotic applications. The topic will cover the issue of planing movement of robotic arm and also controlling and presenting data from mobile robot. This should give more or less understanding what this technology is capable and whats are its current limitations.


\chapter{Introduction to Augmented Reality}
%tutaj można napisac cos o tym gdzie można umiejscowic w przestrzeni augmented reality (relity<->mixed reality<->virtual reality)
The perception of our surroundings is made to a large extent by the organ of sight. Thanks to that we are able to navigate and operate in our real environment. But what if we will try to trick him by placing displays in front of our eyes? Depending on content generated by computer it could simply show some additional information or create illusion of being completely somewhere else. To distinguish types of immersion the concept of a "Reality-Virtuality Continuum" \cite{RVContinuum} was created. It graphical representation is shown on figure \ref{fig:RVContinuum}.

\begin{figure}[!ht]
  \centering
    \includegraphics[width=0.8\textwidth]{RVContinuum}
  \caption{Reality-Virtuality (RV) Continuum \cite{RVContinuum}}
  \label{fig:RVContinuum}
\end{figure}

On the most left side of this line there is our real environment with real objects in no way disturbed by computer graphics. On the other hand on the most right side there is Virtual Reality with fully 3D generated world that could even in example not holding known by us laws of physics. Between two of those is everything with is mixing one part with another. Depending on what balance is, then we will talk about Augmented Reality or Augmented Virtuality. For example when there operator is getting some simulated cues to augment his natural feedback then it is AR. When in virtual world appears some real life objects or persons then it is called AV. Good example for that are modern news where reporters are working at green screen and in television they appear in 3D generated studios. As this paper will only discuss topic of using Augmented Reality in robotics applications that is why technical aspects of Augmented Virtuality or Virtual Reality will be not considered.

\section{Technology overview}
AR devices can take many different forms but way of processing data is almost always the same. In the simplest way it could be explained as following process. First device need to capture an image and localize itself or the user in environment then mix those data with CG objects and at the end display them on a screen. Figure \ref{fig:ARpipeline} presents this pipeline in graphical form with few additional steps.

\begin{figure}[!ht]
  \centering
    \includegraphics[width=0.8\textwidth]{ARPipeline}
  \caption{Simplified AR pipeline \cite{Shipyard}}
  \label{fig:ARpipeline}
\end{figure}

This section will show different types of technologies with where used at implementation of individual elements of this process.

\subsection{Types of image projection}
%wymienic typy projekcji obrazu
There is several way of displaying virtual objects in our environment. Depending on its type they could be more or less immersing for the user.\\

The most popular and the simplest types of projection is video-mixing. It involves the usage of devices equipped with a camera and standard display to present AR content. In most cases smartphones or tablets are used because they have built-in every needed component and they are very portable. Figure \ref{fig:tabletAR} present example of such projection. The great advantage of this approach compared to others is that you can use one device in cooperation with other people, which could result in a significant reduction of operating costs. The disadvantages should be mentioned that the employee is not able to observe the environment and use both hands to do his job. Therefore, this type of projection is most often used in devices used to supervise the operation of machines.

\begin{figure}[!ht]
  \centering
    \includegraphics[width=0.8\textwidth]{tablet_AR}
  \caption{Worker using tablet as video-mixing device \cite{Shipyard}}
  \label{fig:tabletAR}
\end{figure}

Second type of projection is slightly different than first one. It is called spatial display. In this case virtual objects are shown directly on real environment surfaces by usage of digital projectors. It could be realized on two way. We could have hand-held device with will work similar to flashlight or stationary mounted projector. In first case we could for example inspect virtual paths of transportation robots in warehouses or factory by highlighting floors. Also autonomous cars can use their headlights to display information on the road and for instance inform pedestrian that they could safely cross the passage. Stationary projectors are most often used with collaborative robots. They can display for example some cues for worker which item it will pick-up or where it will place it on shared workspace. Presented types of projection could be seen on Figure \ref{fig:spatialAR}.

\begin{figure}[!ht]
\centering
\begin{subfigure}{.4\textwidth}
  \centering
  \includegraphics[width=.9\linewidth]{augmentedprojectors}
  \caption{Augmented projector \cite{augmentedprojectors}}
  \label{fig:augmentedprojectors}
\end{subfigure}%
\begin{subfigure}{.4\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{spatialARcowork}
  \caption{Man and robot coworking \cite{SpatialARCollaborative}}
  \label{fig:spatialARcowork}
\end{subfigure}
\caption{Examples of spatial projection}
\label{fig:spatialAR}
\end{figure}

Next AR display technology which will be presented is optical see-through. Its principle of operation boils down to projecting images on partially reflecting surface. This allow to combine real world view with generated graphics which will appear as floating in space holograms. There are two ways to achieve this goal. Device could use a head-mounted or head-up displays. Figure \ref{fig:seeThroughAR} shows how the projection is carried out with particular case. First type presented at image \ref{fig:headMounted} is commonly used in situations where worker need to have both hands free to do his work. At HMD could be shown some step by step instructions or in case operating with robots, live sensors data. Example from image \ref{fig:headUp} show the most common scenario of usage of HUD. Nowadays they are showing information about speed and navigation guidance for a driver but in autonomous vehicles they could display also some cues what car see and what it intends to do.

\begin{figure}[!ht]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{HMD}
  \caption{Head-Mounted Display}
  \label{fig:headMounted}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{HUD}
  \caption{Head-Up Display \cite{HUD}}
  \label{fig:headUp}
\end{subfigure}
\caption{Examples of optical see-through projections}
\label{fig:seeThroughAR}
\end{figure}

\subsection{Positioning and location}
%rodzaje lokalizacji obiektow w scenie i pozycjonowania samych gogli w przestrzeni
Only small part of AR devices is presenting data that is not depending on real environment obstacles. These include basic versions of HUD and smartglasses. Rest of them need somehow localize itself or detect position of the user. In this section various types of positioning techniques will be presented.\\

Marker-based localization is simplest one. The principle of operation consists in detection of predefined shape or image by the device camera. To achieve this first of all data need to be preprocessed.

Most trivial example is when our marker have square shape and high contrast. In that case algorithm need only know what is length of side. With this information it is able to calculate position and orientation of the device basing on detected corners. As the most important are only the edges the center can have any shape. This allow to store some information inside and also help to determinate from with side of square camera is looking. Figure \ref{fig:ARMarker} show detected marker with read associated to it ID number. Pros of this approach is quite low requirement for computational power and easy way to generate many unique ID's. The disadvantage is that the user must print these images and arrange them in their chosen locations.

There is also possibility to track normal images or even 3D objects. In this case there are used algorithms with are extracting some particular features from provided data (Figure \ref{fig:featureExtraction}). Basing on them device could be able to localize itself in space only by looking on for example machine logo. Pros of this approach is better tracking of object even when whole image is not in camera view. Disadvantage is more complex computation.

\begin{figure}[!ht]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{ARMarker}
  \caption{Detected ArUco marker with ID \cite{ArUco}}
  \label{fig:ARMarker}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.65\linewidth]{featureExtraction}
  \caption{Image with marked extracted features}
  \label{fig:featureExtraction}
\end{subfigure}
\caption{Example of markers used for localization purposes}
\label{fig:markerBasedLocalization}
\end{figure}

Next discussed type of localization is based on measurements obtained via external devices. There are many possible implementations of this approach but they could be categorized based upon the working principle.

Optical tracking of passive markers \cite{MoCap} is best known in film industry. It is used in Motion Capture studios to transfer movement of actor to 3D modeled characters. It requires usage of multiple high-speed cameras with infrared illuminators fixed around the measurement area to triangulate a reflective marker position. Successful capture of tracked point by at least two camera give sub-millimeter precision. To avoid situation where marker is occluded by some obstacles localized object could be equipped with redundant ones. Disadvantage of such system is limitation of operating area caused by strength of the reflected light. There is also a way to increase to a certain extent range by using active markers which are light sources but this cause another problems with need to provide power to those.

From optical localization method there are also systems working without any markers. They are based on 3D depth cameras with are providing not only image but also give information about distance from objects. This allow to not only get position of tracked device but also could give feedback about scanned environment. This kind of systems generally have operation distance from 0.5 to 8 meters. Precision of positioning it is inversely proportional to it.

External localization could be also realized by measuring field strength or time of flight of electromagnetic waves. In this case at least three transmitters are needed to establish position. This method is slightly less accurate in compare to optical ones but have huge advantage in the form of being able to track device without direct visibility. Also range and refresh rate of such systems is significantly larger than optical ones.

\begin{figure}[!ht]
\centering
\begin{subfigure}{.45\textwidth}
  \centering
  \includegraphics[width=.9\linewidth]{MoCapStudio}
  \caption{Motion capture studio \cite{MoCapStudio}}
  \label{fig:MoCapStudio}
\end{subfigure}
\begin{subfigure}{.45\textwidth}
  \centering
  \includegraphics[width=.82\linewidth]{depthData}
  \caption{3D Camera Depth Data \cite{depthData}}
  \label{fig:depthData}
\end{subfigure}\\
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=0.8\linewidth]{DWM1000}
  \caption{DWM1000 UWB Localization module \cite{DWM1000}}
  \label{fig:DWM1000}
\end{subfigure}
\caption{External localization systems}
\label{fig:externalDevicesLocalization}
\end{figure}

Last presented type of positioning is Simultaneous Localization And Mapping (SLAM). This is technique with is using Laser Range Sensors or 3D Depth cameras to create map of environment. Basing on that data algorithm could calculate position and orientation in space. The big advantage of this solution is the ability to integrate all elements into one device with makes it almost limitless in terms of working area. Also additional gain is that the obtained data can be used to classify objects located in the environment. Unfortunately, such calculations takes huge amount of processing power so device need more expensive components to be able run in real-time. At Figure \ref{fig:SLAMHololens} could be seen 3D map of room created by using depth sensor and SLAM algorithm.

\begin{figure}[!ht]
  \centering
    \includegraphics[width=0.9\textwidth]{SLAMHololens}
  \caption{Room mapped using SLAM algorithm with usage of Microsoft Hololens}
  \label{fig:SLAMHololens}
\end{figure}

\subsection{User control and interaction}
%sposoby interakcji uzytkownikow z wirtualnymi obiektami (gesty, głos, kontrolery)
Augmented Reality devices requires different approach to how user should use them. As the worker is no longer limited by his stationary desk there is need to change way of interaction. This section will discuss most common used methods.\\

Simplest HMD devices, where in the field of view we had only displayed text or images there is no need to add special types of interaction. In most cases on frame of device are placed buttons or small touchpad (Figure \ref{fig:swipeGlass}) which allow user to control displayed content. This approach allows to reduce production costs so that it could have lower price tag and be able to get to more customers.

When device is providing ability to observe 3D objects in space then user need some controllers to be able to operate them. On the market most popular ones are using optical tracking (Figure \ref{fig:MRControllers}) but there are also those that uses electromagnetic fields. In addition, all are equipped with 6-DOF sensors to enhance stability of calculated position. This type of input is very precise but requires holding the controller which in some cases may hinder normal work.

The solution to this problem is to recognize gestures and track them (Figure \ref{fig:gestureControl}). This allow to operate with 3D environment while still having both hands free. Detection is carried out using depth cameras. Unfortunately like in case of using SLAM algorithms this method consume a lot of computing power so it is reserved only for the most expensive devices.

The last type of interaction with will be discussed is voice control. It is a very intricate topic due to diversity of humans dialects and accents. With current technology embedded computers can interpret only single words or predefined sentences however, using the resources of cloud services it is possible to extract information from context using natural language processing (NLP) algorithms. The disadvantage of this solution is requirement of continuous access to the Internet and quite slow response.

\begin{figure}[!ht]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{swipeGlass}
  \caption{Touchpad at Google Glass \cite{swipeGlass}}
  \label{fig:swipeGlass}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.65\linewidth]{MRControllers}
  \caption{Mixed Reality Controllers \cite{MRControllers}}
  \label{fig:MRControllers}
\end{subfigure}\\
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{gestureControl}
  \caption{Using gesture with Microsoft Hololens \cite{gestureControl}}
  \label{fig:gestureControl}
\end{subfigure}%
\label{fig:ARInteraction}
\end{figure}

\section{Products available on market}
Augmented reality is still quite young technology. Omitting the fact that it was used in the army and civil aviation since the 1960s, it is now becoming available to industry and individual users. There are more and more companies trying their strength in this sector on the market. They use a variety of image display and interaction technologies, and develop newer and newer solutions. This section will show few chosen AR devices and software's.

\subsection{Hardware devices}
The devices presented at table \ref{tab:hardwareDevices} are currently focused mainly on B2B cooperation. High prices and low maturity of the technology to the consumer market have an impact on this. Also there is no one unified environment between them so all applications are written to fulfill specific case. However, some devices can be bought in normal sales, so it is not a closed market for a regular customer.

\subsection{Software solutions}
The cheapest entry level to AR world is use of dedicated SDK's on computer, smartphone or tablet. On the market there are many ready to use solutions which cover different use cases. Most of them are commercial products, but there are also some open-source one. Table \ref{tab:ARSoftware} shows the collected information about several of them.

%\resizebox{\linewidth}{!}{%

\begin{landscape}
\begin{table}[]
\centering
\resizebox{\linewidth}{!}{%
\begin{tabular}{|c|l|l|l|l|l|l|l|l|l|l|}
\hline
Device name                                                                  & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Type of\\ projection\end{tabular}}                          & \multicolumn{1}{c|}{Hardware}                                                                                            & \multicolumn{1}{c|}{Sensors}                                                                                                                                                       & \multicolumn{1}{c|}{Connectivity}                                                                                  & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Battery\\ (life time)\end{tabular}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Mechanical\\ parameters\end{tabular}}                                       & \multicolumn{1}{c|}{User input}                                                                             & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Operating\\ system\end{tabular}} & \multicolumn{1}{c|}{Capabilities}                                                                                                                & Price   \\ \hline
\begin{tabular}[c]{@{}c@{}}Microsoft \\ Hololens\cite{HololensHW}\end{tabular}                & \begin{tabular}[c]{@{}l@{}}Optical see-through\\ 1920x1200 px per eye\\ 40 degrees view angle\end{tabular} & \begin{tabular}[c]{@{}l@{}}Intel x86 CPU\\ Holographic Processing Unit\\ 2 GB RAM\\ 64 GB storage\end{tabular}           & \begin{tabular}[c]{@{}l@{}}Depth Sensor Camera\\ 2MP Video Camera\\ 4x environment understanding cameras\\ IMU\\ Ambient Light Sensor\\ Microphone\\ On-board Speakers\end{tabular} & \begin{tabular}[c]{@{}l@{}}Wi-Fi 802.11ac\\ Bluetooth® 4.1 LE\\ 3.5mm Jack\\ micro-USB\end{tabular}                & \begin{tabular}[c]{@{}l@{}}4500 mAh\\ 2-3 hours\end{tabular}                       & \begin{tabular}[c]{@{}l@{}}Weight: 579 g\\ Adjustable head band\end{tabular}                                               & \begin{tabular}[c]{@{}l@{}}Gesture input\\ Wireless clicker\\ Voice support\\ Buttons on frame\end{tabular} & Windows™ 10                                                                     & \begin{tabular}[c]{@{}l@{}}Spatial sound\\ Gaze tracking\\ 6-DOF tracking\\ Mapping of environment\\ Teleoperation\end{tabular}                  & \$4500  \\ \hline
\begin{tabular}[c]{@{}c@{}}Epson \\ MOVERIO PRO \\ BT-2000/2200\cite{MOVERIOPRO}\end{tabular} & \begin{tabular}[c]{@{}l@{}}Optical see-through\\ 960x540 px per eye\\ 23 degrees view angle\end{tabular}   & \begin{tabular}[c]{@{}l@{}}TI OMAP 4460 1,2 GHz\\ 1 GB RAM\\ 8 GB internal storage\\ 32 GB external microSD\end{tabular} & \begin{tabular}[c]{@{}l@{}}5MP Stereo Camera\\ GPS\\ IMU\\ Ambient Light Sensor\\ Geomagnetic Sensor\\ Microphone\end{tabular}                                                     & \begin{tabular}[c]{@{}l@{}}Wi-Fi 802.11 b/g/n/a\\ Bluetooth® 3.0 \& BLE\\ 3.5mm Jack\\ micro-USB\end{tabular}      & \begin{tabular}[c]{@{}l@{}}2x 1240 mAh\\ $\sim$4 hours\end{tabular}                & \begin{tabular}[c]{@{}l@{}}Weight: 270-290 g\\ Depending on version:\\ - adjustable head band\\ - rubber band\end{tabular} & \begin{tabular}[c]{@{}l@{}}Wired controller\\ Voice support\end{tabular}                                    & Android™ 4.0.4                                                                  & \begin{tabular}[c]{@{}l@{}}Teleoperation\\ 6-DOF tracking\\ Task guidance\end{tabular}                                                          & \$2000  \\ \hline
\begin{tabular}[c]{@{}c@{}}DAQRI \\ Smart Glasses\cite{DAQRI}\end{tabular}               & \begin{tabular}[c]{@{}l@{}}Optical see-through\\ 1360x768 px per eye\\ 44 degrees view angle\end{tabular}  & \begin{tabular}[c]{@{}l@{}}6th Intel® Core™ m7\\ Dedicated vision processing unit\\ 64 GB storage\end{tabular}           & \begin{tabular}[c]{@{}l@{}}Color Camera\\ AR Tracking Camera\\ Depth Sensor Camera\\ IMU\\ 2 Microphones with\\ Active Noise Cancellation\end{tabular}                             & \begin{tabular}[c]{@{}l@{}}Wi-Fi 802.11 a/b/g/n/ac\\ Bluetooth®\\ 2 USB 3.1 Type C Ports\\ 3.5mm Jack\end{tabular} & 5800 mAh                                                                           & \begin{tabular}[c]{@{}l@{}}Smart Glasses: 335 g\\ Compute Pack: 496 g\\ Adjustable head band\end{tabular}                  & \begin{tabular}[c]{@{}l@{}}Gesture input\\ Voice support\end{tabular}                                       & Custom solution                                                                 & \begin{tabular}[c]{@{}l@{}}Mapping of environment\\ Teleoperation\\ 6-DOF tracking\end{tabular}                                                  & Unknown \\ \hline
\begin{tabular}[c]{@{}c@{}}ARVIND SANJEEV\\ LUMEN\cite{Lumen}\end{tabular}               & \begin{tabular}[c]{@{}l@{}}Projection based\\ 640x360px\\ screen size: 10"-112"\end{tabular}               & \begin{tabular}[c]{@{}l@{}}Broadcom BCM2835 1GHz\\ 512 MB RAM\\ 32 GB external microSD\end{tabular}                      & \begin{tabular}[c]{@{}l@{}}Color Camera\\ Depth Sensor Camera\end{tabular}                                                                                                         & \begin{tabular}[c]{@{}l@{}}Wi-Fi 802.11 b/g/n\\ Bluetooth® 4.1 LE\\ micro-USB\end{tabular}                         & \begin{tabular}[c]{@{}l@{}}1800 mAh\\ $\sim$30 min\end{tabular}                    & Unknown                                                                                                                    & Gesture input                                                                                               & Linux                                                                           & \begin{tabular}[c]{@{}l@{}}Objects recognition\\ Mapping of environment\end{tabular}                                                             & Unknown \\ \hline
\begin{tabular}[c]{@{}c@{}}Light Guide Systems\\ Classic\cite{LGS}\end{tabular}        & Projection based                                                                                           & Any PC                                                                                                                   & Color camera                                                                                                                                                                       & \begin{tabular}[c]{@{}l@{}}HDMI\\ USB\end{tabular}                                                                 & AC Powered                                                                         & Unknown                                                                                                                    & Camera feedback                                                                                             & Windows™                                                                        & \begin{tabular}[c]{@{}l@{}}Augmentation of workstation\\ Task guidance\\ Automatic part assembly check\\ Production time recording\end{tabular} & Unknown \\ \hline
\begin{tabular}[c]{@{}c@{}}Magic Leap\\ One\cite{MagicLeap}\end{tabular}                     & \begin{tabular}[c]{@{}l@{}}Optical see-through\\ 1280x960 px per eye\\ 50 degrees view angle\end{tabular}  & \begin{tabular}[c]{@{}l@{}}CPU: NVIDIA® Parker\\ GPU: NVIDIA Pascal™\\ 8 GB RAM\\ 128 GB Storage\end{tabular}            & \begin{tabular}[c]{@{}l@{}}Depth camera\\ Environment understanding cameras\\ IMU\\ Microphone\\ Onboard Speakers\end{tabular}                                                     & \begin{tabular}[c]{@{}l@{}}Wi-Fi 802.11ac/b/g/n\\ Bluetooth® 4.2\\ USB-C\end{tabular}                              & $\sim$3 hours                                                                      & Weight: 345 g                                                                                                              & \begin{tabular}[c]{@{}l@{}}Gesture input\\ 6-DOF tracked controllers\\ Voice support\end{tabular}           & Lumin OS                                                                        & \begin{tabular}[c]{@{}l@{}}Mapping of environment\\ Teleoperation\\ 6-DOF tracking\end{tabular}                                                  & \$2,295 \\ \hline
\begin{tabular}[c]{@{}c@{}}Google Glass\\ Enterprise Edition\cite{Glass}\end{tabular}    & \begin{tabular}[c]{@{}l@{}}Optical see-through\\ 640×360 px\end{tabular}                                   & \begin{tabular}[c]{@{}l@{}}Intel Atom processor\\ 2 GB RAM\\ 32 GB storage\end{tabular}                                  & \begin{tabular}[c]{@{}l@{}}5 MP camera\\ IMU\\ Ambient Light Sensor\\ GPS \& GLONASS\\ Barometer\\ Bone conduction transducer\end{tabular}                                         & \begin{tabular}[c]{@{}l@{}}Wi-Fi 802.11n/ac\\ Bluetooth®\end{tabular}                                              & \begin{tabular}[c]{@{}l@{}}780 mAh\\ $\sim$8 hours\end{tabular}                    & Weight: 36g                                                                                                                & \begin{tabular}[c]{@{}l@{}}Voice commands\\ Touchpad\\ Mobile app\end{tabular}                              & Android™ 4.4                                                                    & \begin{tabular}[c]{@{}l@{}}Teleoperation\\ Assistant\\ Task guidance\end{tabular}                                                               & \$1,500 \\ \hline
\end{tabular}%
}
\caption{List of selected AR goggles models}
\label{tab:hardwareDevices}
\end{table}
\end{landscape}

\begin{landscape}
\begin{table}[]
\centering
\resizebox{\linewidth}{!}{%
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
Year & 2000 & 2001 & 2002 & 2003 & 2004 & 2005 & 2006 & 2007 & 2008 & 2009 & 2010 \\
\hline
GPD in billions & 235  &  225 bn & 223 bn & 323 & 423  & 523 & 624 & 725 & 826  & 924  & 1022  \\
\hline
\end{tabular}
}
\caption{Placeholder}
\label{tab:ARSoftware}
\end{table}
\end{landscape}

\chapter{Applications of Augmented Reality}
This part of thesis will be presenting some founded on internet publications about using AR in robotics applications. It should give a pretty good look at what this technology is created for and suggest what areas have not yet been researched.

\section{ARDebug}
The first issue raised will be the use of AR to visualize the parameters of individual robotic swarm units basing on research paper \cite{ARDebug} created by scientists from York Robotics Laboratory. The tool presented by them could help to easier find bugs in implemented algorithms by giving real-time visual feedback basing on each robot state.

The configuration of the environment requires the use of a locating system. Researchers for this purposes used an 5 MP camera which was placed 2.5 m above center of operating area and the ArUco markers placed on robots. Then, using the algorithms built into the OpenCV library, they determine their position. The data obtained in this way was packed in the JSON structure with is accepted by ARDebug application. Authors deliberately chose this format to allow easy integration of other location methods.

Another element required by the application is data of the status of each robot. They must also be provided in JSON format over Wi-Fi or Bluetooth to the supervising computer.

After collecting all the information ARDebug imposes on video from camera, data acquired from tracking system and robot. Additionally, it presents them in the form of tables with key value representation and graphs. Example view from this program is presented at figure \ref{fig:ARDebug}.

\begin{figure}[!ht]
  \centering
    \includegraphics[width=0.9\textwidth]{ARDebug}
  \caption{The ARDebug user interface \cite{ARDebug}}
  \label{fig:ARDebug}
\end{figure}

\section{Virtual sensing technology}
Another discussed issue was also created to improve and speed up the testing of algorithms written for the purpose of robotic swarms. This time, the research work \cite{ARGoS} of scientists from Universit ́ Libre de Bruxelles was examined. In this paper they describe created by them system ARGoS with allow to simulate sensors input without having installed real ones. This approach significantly reduces the costs of testing a large number of robots by decreasing the amount of components needed per unit. In addition, it offers the possibility of flexible configuration of the sensors work range without any hardware modifications.

Like in previous system there is also need of reading robots positions. It was realized in the same way, that is, by using a camera suspended above the working area and markers on tracked units. The data collected in this way is used to reproduce the scene in the simulator. Then virtual sensor modules calculate their outputs and provide wireless this information to real robots.

Whole process could be viewed in three perspectives. First one is virtual environment, second AR video-mixing and third by lighting LEDs on real robots. Output of the system could be seen na figure \ref{fig:ARGoS}.

\begin{figure}[!ht]
  \centering
    \includegraphics[width=0.9\textwidth]{ARGoS}
  \caption{The ARGoS sensor simulator \cite{ARGoS}}
  \label{fig:ARGoS}
\end{figure}

\section{Interactive Spatial AR in Collaborative Robot Programming}
Scientists from Brno University of Technology decided to explore the subject of human cooperation with the machine. In their paper \cite{SpatialARCollaborative} described was concept of utilization of shared workspace as a interactive space with could be used to visualize and control movements of collaborative robot. The goal was to create system which will simplify programming to such level that every ordinary skilled worker could customize robot's behavior and adopt it to current work or preferences.

Researchers to test different solutions have created a easy to deploy and modular stand. It consisted of standard workshop table with capacitive touch foil on it, PR2 collaborative robot, Microsoft Kinect V2 depth sensors, two speakers and truss with projector hanging on it. Complete setup could be seen on figure \ref{fig:spatialARdesk}.

\begin{figure}[!ht]
  \centering
    \includegraphics[width=0.9\textwidth]{spatialARdesk}
  \caption{Setup of collaborative workshop desk \cite{SpatialARCollaborative}}
  \label{fig:spatialARdesk}
\end{figure}

Prepared elements changes shared workspace into interactive display. Thanks to this, the employee can mark the work areas of the robot in a very intuitive way and give him instructions by using simple GUI. Also this approach allows him to continuously observe what decisions robot is making without looking away from his job.

The authors carried out research on a group of people to check whether the applied solution introduces a sense of safety when working with collaborative robots. It finds out that almost every participant assessed positively their work, however they also point out that GUI should be improved.

\chapter{Research of the subject}
The aim of the study was to investigate the possibility of using augmented reality in robotics. For this purpose, several issues related to stationary and mobile robots were selected.

In the first case, it was decided to create an user interface that would allow to remotely control the position of the robotic arm using gestures. Thanks to this, working from a safe distance, the operator is able to set a trajectory of motion along the points, simultaneously observing the behavior of the machine.

In the case of mobile robots, the topics of visualization of sensory data and route planning were examined. Displaying in real time sensors reading in user environment could help with debugging some hardware or software issues. Also being able to see waypoints of robot on the floor could help to check if it will not hit any obstacle.

\section{Used technologies}
In order to implement selected tasks, it was decided to use the augmented reality goggles for this purpose. The choice fell on Microsoft Hololens due to the good performance, tracking quality and the ability to loan them for testing. Picture of them could be seen on figure \ref{fig:hololensGoggles}. Technical parameters was presented earlier in the table \ref{tab:hardwareDevices}.

\begin{figure}[!ht]
  \centering
    \includegraphics[width=0.9\textwidth]{hololensGoggles}
  \caption{Picture presenting Microsoft Hololens AR Googles \cite{hololensGoggles}}
  \label{fig:hololensGoggles}
\end{figure}

\subsection{3D Engine}
Application for Hololens could be written in every programming language which has support for Universal Windows Platform API. However, due to the fact that goggles are working in a 3D environment, developer also need a graphics engine to be able to create spatial applications. On the market there are two main competitors which is Unreal Engine and Unity Engine. First one in a subjective sense, is more intuitive and provides a better visuals. Also allow to have more control on code execution, due to possibility of writing backend in C++. Unfortunately it do not have good support from Microsoft side for Hololens that is why it was not chosen to do this tasks.

On the other hand, Unity thanks to a large community is easy to learn. Many tutorials helps programmers quite fast get familiar with user interface (Figure \ref{fig:unityUI}) and give ability to start deploying their first apps, even without writing a single line of code. However, for every non standard behavior need to be created C\# script file.

\begin{figure}[!ht]
  \centering
    \includegraphics[width=0.9\textwidth]{unityUI}
  \caption{Unity Editor main window}
  \label{fig:unityUI}
\end{figure}

Developing applications for Microsoft Hololens in the Unity Engine has additional advantages in terms of the availability of the HoloToolkit library. It allows you to easily use advanced goggles functions such as location or gesture support by using already prepared assets or scripts. The only thing a programmer has to do is pin the function he needs to the object.

\subsection{Vuforia}
Vuforia is an multiplatform Engine which uses advanced computer vision algorithms to bring AR experience to almost every kind of application. Using it, it is possible to locate and track position of device basing on the 2D markers/images or provided 3D models of real objects. Thanks to that user is able to see on the screen elements created in 3D environment in a real world. All features of the engine was presented in table \ref{tab:ARSoftware}

The decision to use this software was made for two reasons. The first one was easy integration with unity environment and second one was possibility to detect not only predefined markers but any kind of image. At figure \ref{fig:featureExtraction} could be seen preprocessed Dobot robotic arm logo with extracted features, which is used in this research.

\subsection{Robotic Operating System}
Both of used in this research robots was controlled by Robotic Operating System. It is open-source platform with provide software libraries and tools for building robot applications.

The principle of ROS operation is based on the idea of running many nodes which each of it is representing some behavior. In example one of them could read some sensor data, process it and at the end share it with others. Communication system between nodes is based on anasynchronous anonymous publish/subscribe mechanism (Figure \ref{fig:ros}). Thanks to that we get a flexible and modular platform which allows us to configure any data flow we want. However, not all operations can be performed in an asynchronous manner. To this end, ROS also provides a synchronous request/response mechanism called services.

\begin{figure}[!ht]
  \centering
    \includegraphics[width=0.9\textwidth]{ros}
  \caption{ROS publish/subscribe mechanism}
  \label{fig:ros}
\end{figure}

Another great feature is also ability to record and playback messages. As the data in system is shared by publish/subscribe mechanism there is easy way to capture them in example on real robots and then replay on offline machine without any changes in code. In combination with tools like rviz we are able to recreate and visualize all robots behaviors without having access to them.

Connection between ROS and external system could be realized by ROSBridge. This library is able to create server which could send and receive data in JSON format. Each frame is containing object name, topic/service name, messageType and optional payload. Such a simple solution allows for easy implementation of the client on any device. That is why this tool was used in the research to be able to exchange data between ROS and Unity Engine.

\section{Test results}

\subsection{Robotic arm}
The tools presented in the previous section were used to implement the task set at the beginning of the chapter. Thanks to the created application, the user is able to move the virtual object in space by means of gestures, which is then followed by the effector of the robotic arm. The selected positions could be memorized and in this way motion sequences could be created.

As mentioned before, the application was created in Unity using the Holotoolkit library. As far as the supplied elements allowed for trouble-free operation of the peripherals provided by goggle sensors, connecting them with ROS required adding one's own module. It was written as a C\# script, in which the websocket connection to the ROSBridge server was opened. Then there was need to create protocol layer which will pack data into proper JSON messages. Their use allowed creating two-way communication between the robot and the application in Unity.

With the ability to exchange data with ROS, implementations of the basic assumptions have started. Initially, the coordinate systems of the robot and goggles had to be synchronized so that the objects oriented in the virtual space would correspond to reality. For this purpose, the aforementioned Vuforia library was used. Created marker need to be placed in exact same position in front of robotic arm as it was setted in Unity 3D environment. Thanks to that all objects will appear just in a right spot.

Next task was to create a 3D cube and bind its position with the position of the robot effector taken from ROS. This test allowed to observe that the coordinate axes of the robot are pointing in other directions than Unity one's. To facilitate the later work, another script dealing with bidirectional transformation was written.

After application was able to receive an position from the robot, a part of the code allowing its setting was written. In this case gesture recognition module from HoloToolkit library was used. It allows to capture event when particular gesture is occuring and extract from it information about relative position of hand. Using this from the perspective of the code, we are able to add a hand translation vector to the position vector of the selected object which will appear as its shift in space. This allow to move previously created cube, then reads it position and at the end write it to robot. That part works very well. On figure \ref{fig:armMove} could be seen screenshots from hololens with different positions setted.

\begin{figure}[!ht]
\centering
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=.9\linewidth]{armMove1}
\end{subfigure}%
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=.9\linewidth]{armMove2}
\end{subfigure}
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=.9\linewidth]{armMove3}
\end{subfigure}%
\caption{Setting robotic arm position by gesture}
\label{fig:armMove}
\end{figure}

Last part of the task was to give ability to store setted positions, draw path between them and at the end be able to automatically replay sequence of movements. For this purpose an click gesture was used. When user set desired position by draging followed by robot object then he could click on it. This event will instantiate a sphere at that place to visualize and store position. In addition, a path is visualized between the next steps to present the movement of the robot.

\subsection{Mobile robot}


\chapter{Summary}
\appendix


\addcontentsline{toc}{chapter}{References} %utworzenie w
                                             %spisietreści pozycji
                                             %Bibliografia

\bibliography{references} % wstawia bibliografię korzystając z pliku
                            % bibliografia.bib - dotyczy BibTeXa,
                            % jeżeli nie korzystamy z BibTeXa należy
                            % użyć otoczenia thebibliography

%opcjonalnie może się tu pojawić spis rysunków i tabel
% \listoffigures
% \listoftables
\end{document}
